{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from stop_words import get_stop_words\n",
    "from collections import Counter\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.gridspec as gridspec \n",
    "\n",
    "\n",
    "color = sns.color_palette()\n",
    "sns.set_style(\"dark\")\n",
    "\n",
    "#nlp\n",
    "import string\n",
    "import re    #for regex\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from nltk import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "# Tweet tokenizer does not split at apostophes which is what we want\n",
    "from nltk.tokenize import TweetTokenizer   \n",
    "\n",
    "\n",
    "#FeatureEngineering\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_is_fitted\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "tokenizer=TweetTokenizer()\n",
    "eng_stopwords = set(stopwords.words(\"english\"))\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "APPO = {\n",
    "\"aren't\" : \"are not\",\n",
    "\"can't\" : \"cannot\",\n",
    "\"couldn't\" : \"could not\",\n",
    "\"didn't\" : \"did not\",\n",
    "\"doesn't\" : \"does not\",\n",
    "\"don't\" : \"do not\",\n",
    "\"hadn't\" : \"had not\",\n",
    "\"hasn't\" : \"has not\",\n",
    "\"haven't\" : \"have not\",\n",
    "\"he'd\" : \"he would\",\n",
    "\"he'll\" : \"he will\",\n",
    "\"he's\" : \"he is\",\n",
    "\"i'd\" : \"I would\",\n",
    "\"i'd\" : \"I had\",\n",
    "\"i'll\" : \"I will\",\n",
    "\"i'm\" : \"I am\",\n",
    "\"isn't\" : \"is not\",\n",
    "\"it's\" : \"it is\",\n",
    "\"it'll\":\"it will\",\n",
    "\"i've\" : \"I have\",\n",
    "\"let's\" : \"let us\",\n",
    "\"mightn't\" : \"might not\",\n",
    "\"mustn't\" : \"must not\",\n",
    "\"shan't\" : \"shall not\",\n",
    "\"she'd\" : \"she would\",\n",
    "\"she'll\" : \"she will\",\n",
    "\"she's\" : \"she is\",\n",
    "\"shouldn't\" : \"should not\",\n",
    "\"that's\" : \"that is\",\n",
    "\"there's\" : \"there is\",\n",
    "\"they'd\" : \"they would\",\n",
    "\"they'll\" : \"they will\",\n",
    "\"they're\" : \"they are\",\n",
    "\"they've\" : \"they have\",\n",
    "\"we'd\" : \"we would\",\n",
    "\"we're\" : \"we are\",\n",
    "\"weren't\" : \"were not\",\n",
    "\"we've\" : \"we have\",\n",
    "\"what'll\" : \"what will\",\n",
    "\"what're\" : \"what are\",\n",
    "\"what's\" : \"what is\",\n",
    "\"what've\" : \"what have\",\n",
    "\"where's\" : \"where is\",\n",
    "\"who'd\" : \"who would\",\n",
    "\"who'll\" : \"who will\",\n",
    "\"who're\" : \"who are\",\n",
    "\"who's\" : \"who is\",\n",
    "\"who've\" : \"who have\",\n",
    "\"won't\" : \"will not\",\n",
    "\"wouldn't\" : \"would not\",\n",
    "\"you'd\" : \"you would\",\n",
    "\"you'll\" : \"you will\",\n",
    "\"you're\" : \"you are\",\n",
    "\"you've\" : \"you have\",\n",
    "\"'re\": \" are\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'll\":\" will\",\n",
    "\"didn't\": \"did not\",\n",
    "\"tryin'\":\"trying\"\n",
    "}\n",
    "\n",
    "def clean(comment):\n",
    "    \"\"\"\n",
    "    This function receives comments and returns clean word-list\n",
    "    \"\"\"\n",
    "    #Convert to lower case , so that Hi and hi are the same\n",
    "    comment=comment.lower()\n",
    "    #remove \\n\n",
    "    comment=re.sub(\"\\\\n\",\" \",comment)\n",
    "    # remove leaky elements like ip,user\n",
    "    comment=re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",\"\",comment)\n",
    "    #removing usernames\n",
    "    comment=re.sub(\"\\[\\[.*\\]\",\"\",comment)\n",
    "    \n",
    "    #Split the sentences into words\n",
    "    words=tokenizer.tokenize(comment)\n",
    "    \n",
    "    # (')aphostophe  replacement (ie)   you're --> you are  \n",
    "    # ( basic dictionary lookup : master dictionary present in a hidden block of code)\n",
    "    words=[APPO[word] if word in APPO else word for word in words]\n",
    "    words=[lem.lemmatize(word, \"v\") for word in words]\n",
    "    words = [w for w in words if not w in eng_stopwords]\n",
    "    \n",
    "    clean_sent=\" \".join(words)\n",
    "    # remove any non alphanum,digit character\n",
    "    return(clean_sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = get_stop_words('english')\n",
    "stop_words_two = [word.upper() for word in stop_words]\n",
    "stop_words += [\"You\", \"like\", \"just\", \"will\", \"know\", \"u\", \"you.\"]\n",
    "stop_words += stop_words_two\n",
    "\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "\n",
    "total = len(df['comment_text'])\n",
    "toxic_dict = {}\n",
    "severe_toxic_dict = {}\n",
    "obscene_dict = {}\n",
    "threat_dict = {}\n",
    "insult_dict = {}\n",
    "for i in range(total):\n",
    "    words = df['comment_text'][i].split(\" \")\n",
    "    for word in words:\n",
    "        if word not in stop_words and word:\n",
    "            if df['toxic'][i]:\n",
    "                if word in toxic_dict:\n",
    "                    toxic_dict[word] += 1\n",
    "                else:\n",
    "                    toxic_dict[word] = 1\n",
    "            if df['severe_toxic'][i]:\n",
    "                if word in severe_toxic_dict:\n",
    "                    severe_toxic_dict[word] += 1\n",
    "                else:\n",
    "                    severe_toxic_dict[word] = 1\n",
    "            if df['obscene'][i]:\n",
    "                if word in obscene_dict:\n",
    "                    obscene_dict[word] += 1\n",
    "                else:\n",
    "                    obscene_dict[word] = 1\n",
    "            if df['threat'][i]:\n",
    "                if word in threat_dict:\n",
    "                    threat_dict[word] += 1\n",
    "                else:\n",
    "                    threat_dict[word] = 1\n",
    "            if df['insult'][i]:\n",
    "                if word in insult_dict:\n",
    "                    insult_dict[word] += 1\n",
    "                else:\n",
    "                    insult_dict[word] = 1\n",
    "\n",
    "                    \n",
    "names = [\"toxic\",\"severe_toxic\", \"obscene\", \"threat\", \"insult\"]\n",
    "dictionaries = [toxic_dict, severe_toxic_dict, obscene_dict, threat_dict, insult_dict]\n",
    "counter = 0\n",
    "for dicts in dictionaries:\n",
    "    print(names[counter])\n",
    "    counter += 1\n",
    "    d = Counter(dicts)\n",
    "    for k, v in d.most_common(10):\n",
    "        print(\"{0}: {1}\".format(k, v))\n",
    "        \n",
    "    \n",
    "\n",
    "lengths = df.comment_text.str.len()\n",
    "lengths.hist()\n",
    "lengths.mean(), lengths.std(), lengths.max(), lengths.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "comments = df[\"comment_text\"].copy()\n",
    "comments = comments.apply(lambda x: clean(x))\n",
    "df.to_csv(r'comments.txt', header=None, index=None, sep=' ', mode='a')\n",
    "\n",
    "\n",
    "text = open(\"comments.txt\").read()\n",
    "\n",
    "wordcloud = WordCloud().generate(text)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "\n",
    "wordcloud = WordCloud(max_font_size=40).generate(text)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\")\n",
    "\n",
    "\n",
    "rowsums=df.iloc[:,2:].sum(axis=1)\n",
    "\n",
    "\n",
    "x=rowsums.value_counts()\n",
    "\n",
    "#plot\n",
    "plt.figure(figsize=(8,4))\n",
    "ax = sns.barplot(x.index, x.values, alpha=0.8,color=color[0])\n",
    "plt.title(\"Tags per comment\")\n",
    "plt.ylabel('Number of Comments', fontsize=12)\n",
    "plt.xlabel('Number of Tags ', fontsize=12)\n",
    "\n",
    "#adding the text labels\n",
    "rects = ax.patches\n",
    "labels = x.values\n",
    "for rect, label in zip(rects, labels):\n",
    "    height = rect.get_height()\n",
    "    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "\n",
    "temp_df=train.iloc[:,2:-1]\n",
    "\n",
    "corr=temp_df.corr()\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(corr,\n",
    "            xticklabels=corr.columns.values,\n",
    "            yticklabels=corr.columns.values, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\")\n",
    "\n",
    "#serperate train and test features\n",
    "train_feats=df.iloc[0:len(train),]\n",
    "#join the tags\n",
    "train_tags=train.iloc[:,2:]\n",
    "train_feats=pd.concat([train_feats,train_tags],axis=1)\n",
    "\n",
    "\n",
    "clean_corpus=train[\"comment_text\"].apply(lambda x :clean(x))\n",
    "\n",
    "\n",
    "tfv = TfidfVectorizer(min_df=200,  max_features=10000, \n",
    "            strip_accents='unicode', analyzer='word',ngram_range=(1,1),\n",
    "            use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "            stop_words = 'english')\n",
    "tfv.fit(clean_corpus)\n",
    "features = np.array(tfv.get_feature_names())\n",
    "\n",
    "train_unigrams =  tfv.transform(clean_corpus.iloc[:train.shape[0]])\n",
    "\n",
    "\n",
    "\n",
    "#https://buhrmann.github.io/tfidf-analysis.html\n",
    "def top_tfidf_feats(row, features, top_n=25):\n",
    "    ''' Get top n tfidf values in row and return them with their corresponding feature names.'''\n",
    "    topn_ids = np.argsort(row)[::-1][:top_n]\n",
    "    top_feats = [(features[i], row[i]) for i in topn_ids]\n",
    "    df = pd.DataFrame(top_feats)\n",
    "    df.columns = ['feature', 'tfidf']\n",
    "    return df\n",
    "\n",
    "def top_feats_in_doc(Xtr, features, row_id, top_n=25):\n",
    "    ''' Top tfidf features in specific document (matrix row) '''\n",
    "    row = np.squeeze(Xtr[row_id].toarray())\n",
    "    return top_tfidf_feats(row, features, top_n)\n",
    "\n",
    "def top_mean_feats(Xtr, features, grp_ids, min_tfidf=0.1, top_n=25):\n",
    "    ''' Return the top n features that on average are most important amongst documents in rows\n",
    "        indentified by indices in grp_ids. '''\n",
    "    \n",
    "    D = Xtr[grp_ids].toarray()\n",
    "\n",
    "    D[D < min_tfidf] = 0\n",
    "    tfidf_means = np.mean(D, axis=0)\n",
    "    return top_tfidf_feats(tfidf_means, features, top_n)\n",
    "\n",
    "# modified for multilabel milticlass\n",
    "def top_feats_by_class(Xtr, features, min_tfidf=0.1, top_n=20):\n",
    "    ''' Return a list of dfs, where each df holds top_n features and their mean tfidf value\n",
    "        calculated across documents with the same class label. '''\n",
    "    dfs = []\n",
    "    cols=train_tags.columns\n",
    "    for col in cols:\n",
    "        ids = train_tags.index[train_tags[col]==1]\n",
    "        feats_df = top_mean_feats(Xtr, features, ids, min_tfidf=min_tfidf, top_n=top_n)\n",
    "        feats_df.label = label\n",
    "        dfs.append(feats_df)\n",
    "    return dfs\n",
    "\n",
    "\n",
    "#get top n for unigrams\n",
    "tfidf_top_n_per_lass=top_feats_by_class(train_unigrams,features)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(16,22))\n",
    "plt.suptitle(\"TF_IDF Top words per class(unigrams)\",fontsize=20)\n",
    "gridspec.GridSpec(4,2)\n",
    "plt.subplot2grid((4,2),(0,0))\n",
    "sns.barplot(tfidf_top_n_per_lass[0].feature.iloc[0:9],tfidf_top_n_per_lass[0].tfidf.iloc[0:9],color=color[0])\n",
    "plt.title(\"Toxic Comments\",fontsize=15)\n",
    "plt.xlabel('Word', fontsize=12)\n",
    "plt.ylabel('TF-IDF score', fontsize=12)\n",
    "\n",
    "plt.subplot2grid((4,2),(0,1))\n",
    "sns.barplot(tfidf_top_n_per_lass[1].feature.iloc[0:9],tfidf_top_n_per_lass[1].tfidf.iloc[0:9],color=color[1])\n",
    "plt.title(\"Severely toxic Comments\",fontsize=15)\n",
    "plt.xlabel('Word', fontsize=12)\n",
    "plt.ylabel('TF-IDF score', fontsize=12)\n",
    "\n",
    "\n",
    "plt.subplot2grid((4,2),(1,0))\n",
    "sns.barplot(tfidf_top_n_per_lass[2].feature.iloc[0:9],tfidf_top_n_per_lass[2].tfidf.iloc[0:9],color=color[2])\n",
    "plt.title(\"Obscene Comments\",fontsize=15)\n",
    "plt.xlabel('Word', fontsize=12)\n",
    "plt.ylabel('TF-IDF score', fontsize=12)\n",
    "\n",
    "\n",
    "plt.subplot2grid((4,2),(1,1))\n",
    "sns.barplot(tfidf_top_n_per_lass[3].feature.iloc[0:9],tfidf_top_n_per_lass[3].tfidf.iloc[0:9],color=color[3])\n",
    "plt.title(\"Threat Comments\",fontsize=15)\n",
    "plt.xlabel('Word', fontsize=12)\n",
    "plt.ylabel('TF-IDF score', fontsize=12)\n",
    "\n",
    "\n",
    "plt.subplot2grid((4,2),(2,0))\n",
    "sns.barplot(tfidf_top_n_per_lass[4].feature.iloc[0:9],tfidf_top_n_per_lass[4].tfidf.iloc[0:9],color=color[4])\n",
    "plt.title(\"Insulting Comments\",fontsize=15)\n",
    "plt.xlabel('Word', fontsize=12)\n",
    "plt.ylabel('TF-IDF score', fontsize=12)\n",
    "\n",
    "\n",
    "plt.subplot2grid((4,2),(2,1))\n",
    "sns.barplot(tfidf_top_n_per_lass[5].feature.iloc[0:9],tfidf_top_n_per_lass[5].tfidf.iloc[0:9],color=color[5])\n",
    "plt.title(\"Identity Hate Comments\",fontsize=15)\n",
    "plt.xlabel('Word', fontsize=12)\n",
    "plt.ylabel('TF-IDF score', fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MultinomialNB base line\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, matthews_corrcoef, confusion_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "\n",
    "def train(col):\n",
    "    df = pd.read_csv(\"train.csv\")\n",
    "    y_train = df[col].copy()\n",
    "    X_train = df[\"comment_text\"].copy()\n",
    "    count_vect = CountVectorizer()\n",
    "    X_train_counts = count_vect.fit_transform(X_train)\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "    train_x, test_x, train_y, test_y = train_test_split(X_train_tfidf, y_train, test_size=0.33, random_state=42)\n",
    "    clf = MultinomialNB().fit(train_x, train_y)\n",
    "    y_pred = clf.predict(test_x)\n",
    "    not_col = \"not \" + col\n",
    "    tn, fp, fn, tp = confusion_matrix(test_y, y_pred).ravel()\n",
    "    print(\"-----Specificity---\")\n",
    "    print(tn, fp, fn, tp)\n",
    "    print(\"----Matthew Correlation Coeffecient---\")\n",
    "    print(matthews_corrcoef(test_y, y_pred))\n",
    "    print(\"---Precision and Recall---\")\n",
    "    print(classification_report(test_y, y_pred, target_names=[col, not_col]))\n",
    "\n",
    "total = 0\n",
    "cols = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "\n",
    "for col in cols:\n",
    "    train(col)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multinomial Naive Bayes (Data Preperation)\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import numpy as np\n",
    "from scipy.sparse import hstack\n",
    "import scipy\n",
    "from sklearn.metrics import accuracy_score, classification_report, matthews_corrcoef, confusion_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "def train_clean(col, train_features, y_train):    \n",
    "    train_x, test_x, train_y, test_y = train_test_split(train_features, y_train, test_size=0.33, random_state=42)\n",
    "    clf = MultinomialNB().fit(train_x, train_y)\n",
    "    y_pred = clf.predict(test_x)\n",
    "    not_col = \"not \" + col\n",
    "    tn, fp, fn, tp = confusion_matrix(test_y, y_pred).ravel()\n",
    "    print(\"-----Specificity---\")\n",
    "    print(tn, fp, fn, tp)\n",
    "    print(\"----Matthew Correlation Coeffecient---\")\n",
    "    print(matthews_corrcoef(test_y, y_pred))\n",
    "    print(\"---Precision and Recall---\")\n",
    "    print(classification_report(test_y, y_pred, target_names=[col, not_col]))\n",
    "    \n",
    "df = pd.read_csv(\"train.csv\")\n",
    "X_train = df[\"comment_text\"].copy()\n",
    "X_train = X_train.apply(lambda x : clean(x))\n",
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 1),\n",
    "    max_features=10000)\n",
    "word_vectorizer.fit(X_train)\n",
    "train_word_features = word_vectorizer.transform(X_train)\n",
    "\n",
    "train_upper = X_train.apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "train_unique = X_train.apply(lambda x: len(set(str(x).split())))\n",
    "train_punc = X_train.apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "train_stop = X_train.apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "bad_words_list = requests.get(\"https://gist.githubusercontent.com/ryanlewis/a37739d710ccdb4b406d/raw/0fbd315eb2900bb736609ea894b9bde8217b991a/google_twunter_lol\").text\n",
    "train_bad_words = X_train.apply(lambda x: len([w for w in str(x).split() if w in bad_words_list]))\n",
    "\n",
    "word_features = pd.DataFrame(train_word_features.toarray())\n",
    "\n",
    "dataframes = [word_features, train_upper, train_unique, train_punc, train_stop, train_bad_words]\n",
    "train_features = pd.concat(dataframes, axis=1)\n",
    "\n",
    "cols = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "for col in cols:\n",
    "    print('fit ', col)\n",
    "    y_train = df[col].copy()\n",
    "    train_clean(col, train_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression (Data Preperation)\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import numpy as np\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.metrics import accuracy_score, classification_report, matthews_corrcoef, confusion_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "def train_clean(col, train_features, y_train):    \n",
    "    train_x, test_x, train_y, test_y = train_test_split(train_features, y_train, test_size=0.33, random_state=42)\n",
    "    clf = LogisticRegression(C=0.1, solver='sag').fit(train_x, train_y)\n",
    "    y_pred = clf.predict(test_x)\n",
    "    not_col = \"not \" + col\n",
    "    tn, fp, fn, tp = confusion_matrix(test_y, y_pred).ravel()\n",
    "    print(\"-----Specificity---\")\n",
    "    print(tn, fp, fn, tp)\n",
    "    print(\"----Matthew Correlation Coeffecient---\")\n",
    "    print(matthews_corrcoef(test_y, y_pred))\n",
    "    print(\"---Precision and Recall---\")\n",
    "    print(classification_report(test_y, y_pred, target_names=[col, not_col]))\n",
    "    \n",
    "\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "X_train = df[\"comment_text\"].copy()\n",
    "X_train = X_train.apply(lambda x : clean(x))\n",
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 1),\n",
    "    max_features=10000)\n",
    "word_vectorizer.fit(X_train)\n",
    "train_word_features = word_vectorizer.transform(X_train)\n",
    "\n",
    "train_upper = X_train.apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "train_unique = X_train.apply(lambda x: len(set(str(x).split())))\n",
    "train_punc = X_train.apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "train_stop = X_train.apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "bad_words_list = requests.get(\"https://gist.githubusercontent.com/ryanlewis/a37739d710ccdb4b406d/raw/0fbd315eb2900bb736609ea894b9bde8217b991a/google_twunter_lol\").text\n",
    "train_bad_words = X_train.apply(lambda x: len([w for w in str(x).split() if w in bad_words_list]))\n",
    "\n",
    "word_features = pd.DataFrame(train_word_features.toarray())\n",
    "\n",
    "dataframes = [word_features, train_upper, train_unique, train_punc, train_stop, train_bad_words]\n",
    "train_features = pd.concat(dataframes, axis=1)\n",
    "\n",
    "\n",
    "cols = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "for col in cols:\n",
    "    print('fit ', col)\n",
    "    y_train = df[col].copy()\n",
    "    train_clean(col, train_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SGDClassifier (Data Preperation)\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import numpy as np\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.metrics import accuracy_score, classification_report, matthews_corrcoef, confusion_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "def train_clean(col, train_features, y_train):    \n",
    "    train_x, test_x, train_y, test_y = train_test_split(train_features, y_train, test_size=0.33, random_state=42)\n",
    "    clf = SGDClassifier(loss='hinge', penalty='l2',  alpha=1e-3, n_iter=5, random_state=42).fit(train_x, train_y)\n",
    "    y_pred = clf.predict(test_x)\n",
    "    not_col = \"not \" + col\n",
    "    tn, fp, fn, tp = confusion_matrix(test_y, y_pred).ravel()\n",
    "    print(\"-----Specificity---\")\n",
    "    print(tn, fp, fn, tp)\n",
    "    print(\"----Matthew Correlation Coeffecient---\")\n",
    "    print(matthews_corrcoef(test_y, y_pred))\n",
    "    print(\"---Precision and Recall---\")\n",
    "    print(classification_report(test_y, y_pred, target_names=[col, not_col]))\n",
    "    \n",
    "\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "X_train = df[\"comment_text\"].copy()\n",
    "X_train = X_train.apply(lambda x : clean(x))\n",
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 1),\n",
    "    max_features=10000)\n",
    "word_vectorizer.fit(X_train)\n",
    "train_word_features = word_vectorizer.transform(X_train)\n",
    "\n",
    "train_upper = X_train.apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "train_unique = X_train.apply(lambda x: len(set(str(x).split())))\n",
    "train_punc = X_train.apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "train_stop = X_train.apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "bad_words_list = requests.get(\"https://gist.githubusercontent.com/ryanlewis/a37739d710ccdb4b406d/raw/0fbd315eb2900bb736609ea894b9bde8217b991a/google_twunter_lol\").text\n",
    "train_bad_words = X_train.apply(lambda x: len([w for w in str(x).split() if w in bad_words_list]))\n",
    "\n",
    "word_features = pd.DataFrame(train_word_features.toarray())\n",
    "\n",
    "dataframes = [word_features, train_upper, train_unique, train_punc, train_stop, train_bad_words]\n",
    "train_features = pd.concat(dataframes, axis=1)\n",
    "\n",
    "cols = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "for col in cols:\n",
    "    print('fit ', col)\n",
    "    y_train = df[col].copy()\n",
    "    train_clean(col, train_features, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
